{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_test = ['testin', 'testnz', 'test', 'dev']\n",
    "\n",
    "test_manif = {t: f'../data/CommonVoice_dataset/splits/for_notebooks/{t}.csv' for t in to_test}\n",
    "MODEL_TO_LOAD = 'saved/BEST_fm_2019-05-02_09-07-38.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from warpctc_pytorch import CTCLoss\n",
    "\n",
    "from time import time\n",
    "import inspect, sys, os, json\n",
    "from os.path import dirname, abspath\n",
    "sys.path.append(dirname(dirname(abspath(inspect.getfile(inspect.currentframe())))))\n",
    "\n",
    "from pathlib import Path\n",
    "from os import makedirs\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.data_loader import create_binarizer, get_accents_counts\n",
    "from utils import count_parameters\n",
    "from models.modules import MaskConv, SequenceWise, BatchRNN, InferenceBatchSoftmax, Lookahead, \\\n",
    "                    supported_rnns, supported_rnns_inv\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import math\n",
    "\n",
    "from multitask_loss import MtLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from decoder import GreedyDecoder, BeamCTCDecoder\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport parameters\n",
    "\n",
    "param = parameters.get_parameters(dev=False, epochs=2, us_en=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_cnts(list_):\n",
    "    return pd.Series(list_).value_counts()\n",
    "\n",
    "def extract_num (s):\n",
    "    return ''.join([c if c.isdigit() else '' for c in s])\n",
    "\n",
    "def ids_list(manifest):\n",
    "    ids = []\n",
    "    with open(manifest) as f:\n",
    "        for l in f:\n",
    "            s = l.split('/')\n",
    "            ids.append(f'{s[3]}-{s[5].split(\".\")[0]}')\n",
    "    return ids\n",
    "\n",
    "def make_accent_dict(manifest_path):\n",
    "    accent_dict = {}\n",
    "    class_dict = {}\n",
    "    with open(manifest_path) as f:\n",
    "        for l in f:\n",
    "            wav, txt, acc = l.split(',')\n",
    "            num = extract_num(wav)\n",
    "            accent = acc.strip()\n",
    "            if accent not in class_dict:\n",
    "                new_key = 0 if (len(class_dict) == 0) else max(class_dict.values()) + 1\n",
    "                class_dict[accent] = new_key\n",
    "            accent_dict[num] = class_dict[accent]\n",
    "    return accent_dict, {v: k for k, v in class_dict.items()}\n",
    "\n",
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n",
    "    return torch.index_select(a, dim, order_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaldiDeepspeechDataset(Dataset):\n",
    "    \"\"\"Defines an iterator over the dataset. This class is intended to be used with PyTorch DataLoader\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, labels, sample_ids, transcripts_path,\n",
    "                 accent_id_dict, ivectors_path=None):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.ivectors_path = ivectors_path\n",
    "        self.transcripts_path = transcripts_path\n",
    "        self.accent_id_dict = accent_id_dict\n",
    "        self.labels_map = dict([(labels[i], i) for i in range(len(labels))])\n",
    "        if isinstance(sample_ids, list):\n",
    "            self._datafiles = sample_ids\n",
    "        else:\n",
    "            with open(sample_ids) as f:\n",
    "                self._datafiles = [x.strip() for x in f.readlines()]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        file_idx = self._datafiles[index]\n",
    "        \n",
    "        if 'dev' in file_idx:\n",
    "            transcripts_path = '../data/CommonVoice_dataset/cv-valid-dev/txt/'\n",
    "            data_path = '../data/CommonVoice_dataset/kaldi/dev-norm'\n",
    "        elif 'train' in file_idx:\n",
    "            transcripts_path = '../data/CommonVoice_dataset/cv-valid-train/txt/'\n",
    "            data_path = '../data/CommonVoice_dataset/kaldi/train-norm'\n",
    "        elif 'test' in file_idx:\n",
    "            transcripts_path = '../data/CommonVoice_dataset/cv-valid-test/txt/'\n",
    "            data_path = '../data/CommonVoice_dataset/kaldi/test-norm'\n",
    "            \n",
    "        with open(os.path.join(data_path, file_idx)) as f:\n",
    "            sample = json.load(f)\n",
    "        sample = torch.FloatTensor(sample)\n",
    "        \n",
    "        target = self.accent_id_dict[extract_num(self._datafiles[index])]\n",
    "        \n",
    "        s_id = file_idx.split('-')[-1]\n",
    "\n",
    "        transcript_path = f'{transcripts_path}sample-{s_id}.txt'\n",
    "        transcript = self.parse_transcript(transcript_path)             \n",
    "        \n",
    "        if self.ivectors_path is None:\n",
    "            return torch.FloatTensor(sample), target, transcript\n",
    "        else:\n",
    "            with open(os.path.join(self.ivectors_path, self._datafiles[index])) as f:\n",
    "                ivect = json.load(f)\n",
    "            return torch.FloatTensor(sample), target, transcript, torch.FloatTensor(ivect)\n",
    "        \n",
    "    def parse_transcript(self, transcript_path):\n",
    "        with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "            transcript = transcript_file.read().replace('\\n', '')\n",
    "        transcript = list(filter(None, [self.labels_map.get(x) for x in list(transcript)]))\n",
    "        return transcript\n",
    "                      \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self._datafiles)\n",
    "    \n",
    "def collate_fn(batch_tot):\n",
    "    \"\"\"This function takes list of samples and assembles a batch. It is intended to used in PyTorch DataLoader.\"\"\"\n",
    "    batch = list(zip(*batch_tot))\n",
    "    ivect = None\n",
    "    \n",
    "    if len(batch) == 3:\n",
    "        input_, acc, trs = batch\n",
    "    elif len(batch) == 4:\n",
    "        input_, acc, trs, ivect = batch\n",
    "\n",
    "    input_lens = torch.tensor([len(r) for r in input_])\n",
    "    acc = torch.tensor(acc)\n",
    "    \n",
    "    input_ = nn.utils.rnn.pad_sequence(input_, batch_first=True)\n",
    "\n",
    "    target_lens = torch.tensor([len(t) for t in trs])\n",
    "\n",
    "    if ivect is not None:\n",
    "        ivect = nn.utils.rnn.pad_sequence(ivect, batch_first=True)\n",
    "        ivect = tile(ivect, 1, 10)\n",
    "        ivect = ivect[:, :input_.size(1), :]\n",
    "        input_ = torch.cat([input_, ivect], dim=2)\n",
    "    \n",
    "    __, idx = input_lens.sort(descending=True)\n",
    "    \n",
    "    targets = np.array(trs)[idx]\n",
    "    targets = torch.tensor([t for target in targets for t in target])\n",
    "\n",
    "    return input_[idx], input_lens[idx].int(), targets.int(), target_lens[idx].int(), acc[idx].int()\n",
    "\n",
    "class KaldiDeepspeechDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a data loader for SpeechDatasets.\n",
    "        \"\"\"\n",
    "        super(KaldiDeepspeechDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loader(test_manif):\n",
    "    test_dict, __ = make_accent_dict(test_manif)\n",
    "\n",
    "    test_dataset = KaldiDeepspeechDataset(data_path=param['test_kaldi'],\n",
    "                                  labels=param['labels'],\n",
    "                                  sample_ids=ids_list(test_manif), \n",
    "                                  transcripts_path=param['test_transcripts'],\n",
    "                                  accent_id_dict=test_dict,\n",
    "                                  ivectors_path=None)\n",
    "\n",
    "    test_loader = KaldiDeepspeechDataLoader(test_dataset, \n",
    "                                    shuffle=True, \n",
    "                                    num_workers=param['num_worker'],\n",
    "                                    batch_size=param['batch_size'])\n",
    "    return test_loader\n",
    "    \n",
    "test_loaders = {k: get_test_loader(v) for k, v in test_manif.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, \n",
    "                rnn_type=nn.LSTM, \n",
    "                labels=\"abc\", \n",
    "                rnn_hidden_size=768, \n",
    "                nb_layers=5, \n",
    "                bidirectional=True,\n",
    "                DEBUG=False):\n",
    "\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        # model metadata needed for serialization/deserialization\n",
    "        self._DEBUG = DEBUG\n",
    "        self._hidden_size = rnn_hidden_size\n",
    "        self._nb_layers = nb_layers\n",
    "        self._rnn_type = rnn_type\n",
    "        self._labels = labels\n",
    "        self._bidirectional = bidirectional\n",
    "\n",
    "        self.conv = MaskConv(nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True)\n",
    "        ))\n",
    "\n",
    "        rnn_input_size = 320\n",
    "\n",
    "        rnns = []\n",
    "        rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "                       bidirectional=bidirectional, batch_norm=False)\n",
    "        rnns.append(('0', rnn))\n",
    "        for x in range(nb_layers - 1):\n",
    "            rnn = BatchRNN(input_size=rnn_hidden_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "                           bidirectional=bidirectional)\n",
    "            rnns.append(('%d' % (x + 1), rnn))\n",
    "        self.rnns = nn.Sequential(OrderedDict(rnns))\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        if self._DEBUG:\n",
    "            print('')\n",
    "            print('# BEGIN HEAD #')\n",
    "            print('input', x.size())\n",
    "\n",
    "        lengths = lengths.cpu().int()\n",
    "        output_lengths = self.get_seq_lens(lengths)\n",
    "        \n",
    "        x = x.view(x.size(0), 1, x.size(1), x.size(2))\n",
    "        x = x.transpose(2, 3)\n",
    "        if self._DEBUG:\n",
    "            print('after view transpose', x.size())\n",
    "            \n",
    "        x, _ = self.conv(x, output_lengths)\n",
    "        if self._DEBUG:\n",
    "            print('after conv', x.size())\n",
    "\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Collapse feature dimension\n",
    "        x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
    "        if self._DEBUG:\n",
    "            print('after view transpose', x.size())\n",
    "\n",
    "        for rnn in self.rnns:\n",
    "            x = rnn(x, output_lengths)\n",
    "        if self._DEBUG:\n",
    "            print('after rnn', x.size())\n",
    "    \n",
    "        self._DEBUG = False\n",
    "        return x, output_lengths\n",
    "\n",
    "    def get_seq_lens(self, input_length):\n",
    "        \"\"\"\n",
    "        Given a 1D Tensor or Variable containing integer sequence lengths, return a 1D tensor or variable\n",
    "        containing the size sequences that will be output by the network.\n",
    "        :param input_length: 1D Tensor\n",
    "        :return: 1D Tensor scaled by model\n",
    "        \"\"\"\n",
    "        seq_len = input_length\n",
    "        for m in self.conv.modules():\n",
    "            if type(m) == nn.modules.conv.Conv2d:\n",
    "                seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n",
    "        return seq_len.int()\n",
    "    \n",
    "class FC(nn.Module):\n",
    "    def __init__(self, \n",
    "                 rnn_hidden_size=768,\n",
    "                 labels='abc',\n",
    "                 DEBUG=False,):\n",
    "    \n",
    "        super(FC, self).__init__()\n",
    "\n",
    "        self._DEBUG = DEBUG\n",
    "        self._labels = labels\n",
    "        num_classes = len(self._labels)\n",
    "\n",
    "        fully_connected = nn.Sequential(\n",
    "            nn.BatchNorm1d(rnn_hidden_size),\n",
    "            nn.Linear(rnn_hidden_size, num_classes, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            SequenceWise(fully_connected),\n",
    "        )\n",
    "        self.inference_softmax = InferenceBatchSoftmax()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self._DEBUG:\n",
    "            print('')\n",
    "            print('# BEGIN FC #')\n",
    "            print('input', x.size())\n",
    "        x = self.fc(x)\n",
    "        if self._DEBUG:\n",
    "            print('after fc', x.size())\n",
    "        \n",
    "        x = x.transpose(0, 1)\n",
    "        if self._DEBUG:\n",
    "            print('after transpose', x.size())\n",
    "        # identity in training mode, softmax in eval mode\n",
    "        x = self.inference_softmax(x)\n",
    "        if self._DEBUG:\n",
    "            print('after softmax', x.size())\n",
    "            \n",
    "        x = x.transpose(0, 1)\n",
    "        if self._DEBUG:\n",
    "            print('after transpose', x.size())\n",
    "            \n",
    "        self._DEBUG = False\n",
    "        return x\n",
    "    \n",
    "class AccentClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 accents={},\n",
    "                 DEBUG=False,):\n",
    "        \n",
    "        super(AccentClassifier, self).__init__()\n",
    "        \n",
    "        num_classes = len(accents)\n",
    "        self._DEBUG = DEBUG\n",
    "                \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_size),\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):     \n",
    "        if self._DEBUG:\n",
    "            print('')\n",
    "            print('# BEGIN Acc #')\n",
    "            print('input', x.size())\n",
    "            \n",
    "        x = x.mean(dim=0)\n",
    "        \n",
    "        if self._DEBUG:\n",
    "            print('after mean', x.size())\n",
    "            \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if self._DEBUG:\n",
    "            print('after fc', x.size())\n",
    "            \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        if self._DEBUG:\n",
    "            print('after softmax', x.size())\n",
    "            \n",
    "        self._DEBUG = False\n",
    "        return x\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(self, \n",
    "                rnn_type=nn.LSTM, \n",
    "                labels=\"abc\", \n",
    "                accents={},\n",
    "                rnn_hidden_size=768, \n",
    "                nb_layers=5, \n",
    "                audio_conf=None,\n",
    "                bidirectional=True,\n",
    "                DEBUG=False):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "            \n",
    "        self.Head = Head(rnn_type=rnn_type, \n",
    "                        labels=labels, \n",
    "                        rnn_hidden_size=rnn_hidden_size, \n",
    "                        nb_layers=nb_layers, \n",
    "                        bidirectional=bidirectional,\n",
    "                        DEBUG=DEBUG)\n",
    "            \n",
    "        self.FC = FC(rnn_hidden_size=rnn_hidden_size,\n",
    "                     labels=labels,\n",
    "                     DEBUG=DEBUG,)\n",
    "        \n",
    "        self.Acc = AccentClassifier(input_size=800,\n",
    "                                  accents=accents,\n",
    "                                  DEBUG=DEBUG)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x, out_len = self.Head(x, lengths)\n",
    "        x_fc = self.FC(x)\n",
    "        x_acc = self.Acc(x)\n",
    "        return x_fc, x_acc, out_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/anaconda3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type Head. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type FC. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type AccentClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(MODEL_TO_LOAD)\n",
    "\n",
    "if param['cuda']:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=param['lr'][0])\n",
    "\n",
    "crit_fc, crit_acc = CTCLoss(), nn.CrossEntropyLoss()\n",
    "\n",
    "decoder = BeamCTCDecoder(param['labels'], lm_path=param['lm_path'],\n",
    "                        alpha=0.8, beta=1.,\n",
    "                        cutoff_top_n=40, cutoff_prob=1.0,\n",
    "                        beam_width=100, num_processes=param['num_worker'])\n",
    "\n",
    "target_decoder = GreedyDecoder(param['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wer(targets, targets_len, out, output_len):\n",
    "    split_targets = []\n",
    "    offset = 0\n",
    "    for size in targets_len:\n",
    "        split_targets.append(targets[offset:offset + size])\n",
    "        offset += size\n",
    "        \n",
    "    decoded_output, _ = decoder.decode(out.data.transpose(0,1), output_len)\n",
    "    target_strings = target_decoder.convert_to_strings(split_targets)\n",
    "    \n",
    "    if False:\n",
    "        print('targets', targets)\n",
    "        print('split_targets', split_targets)\n",
    "        print('out', out)\n",
    "        print('output_len', output_len)\n",
    "        print('decoded', decoded_output)\n",
    "        print('target', target_strings)\n",
    "    \n",
    "    wer, cer = 0, 0\n",
    "    for x in range(len(target_strings)):\n",
    "        transcript, reference = decoded_output[x][0], target_strings[x][0]\n",
    "        wer += decoder.wer(transcript, reference) / float(len(reference.split()))\n",
    "        #cer += decoder.cer(transcript, reference) / float(len(reference))\n",
    "    wer /= len(target_strings)\n",
    "    return wer * 100\n",
    "\n",
    "def check_acc(targets, out):\n",
    "    out_arg = np.argmax(out, axis=1)\n",
    "    diff = torch.eq(out_arg.int(), targets)\n",
    "    acc = torch.sum(diff).float()\n",
    "    return acc / len(targets) * 100\n",
    "\n",
    "def get_loss(inputs, inputs_len, targets, targets_len, target_accents, mix):\n",
    "    \n",
    "    inputs = inputs.cuda()\n",
    "    inputs_len = inputs_len.cuda()\n",
    "    targets = targets.cuda()\n",
    "    targets_len = targets_len.cuda()\n",
    "\n",
    "    # Forward pass\n",
    "    out_fc, out_acc, output_len = model(inputs, inputs_len)\n",
    "\n",
    "    out_fc = out_fc.cpu()\n",
    "    targets = targets.cpu()\n",
    "    targets_len = targets_len.cpu()\n",
    "    out_acc = out_acc.cpu()\n",
    "\n",
    "    if False:\n",
    "        print('## Outputs train')\n",
    "        print('out', out.size())\n",
    "        print('targets', targets.size())\n",
    "        print('output_len', output_len.size())\n",
    "        print('targets_len', targets_len.size())\n",
    "\n",
    "    loss_fc = crit_fc(out_fc, targets, output_len, targets_len)\n",
    "    loss_acc = crit_acc(out_acc, target_accents.long()) * 3000\n",
    "    loss = mix * loss_fc + (1 - mix) * loss_acc\n",
    "    \n",
    "    total_loss = (loss, loss_fc, loss_acc)\n",
    "    return total_loss, out_fc, out_acc, output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader):\n",
    "    mix=0.8\n",
    "    DEBUG=False\n",
    "\n",
    "    model.eval()\n",
    "    epoch_val_losses = []\n",
    "    epoch_wer = []\n",
    "    epoch_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, total=len(test_loader)): ## ## \n",
    "            inputs, inputs_len, targets, targets_len, target_accents = data\n",
    "            total_loss, out_fc, out_acc, output_len = get_loss(*data, mix=mix)\n",
    "            val_loss, __, __ = total_loss\n",
    "\n",
    "            if DEBUG:\n",
    "                print('val loss', val_loss)\n",
    "\n",
    "            epoch_val_losses.append(val_loss)\n",
    "\n",
    "            wer = check_wer(targets, targets_len, out_fc, output_len)\n",
    "            epoch_wer.append(wer)\n",
    "\n",
    "            acc = check_acc(target_accents, out_acc)\n",
    "            epoch_acc.append(acc)\n",
    "\n",
    "    epoch_val_loss = sum(epoch_val_losses) / len(epoch_val_losses) ##\n",
    "    epoch_wer = sum(epoch_wer) / len(epoch_wer)\n",
    "    epoch_acc = sum(epoch_acc) / len(epoch_acc)\n",
    "    return epoch_wer, epoch_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83937850aacd4ad8aa5a4d854391fc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbade46d599b426687216118e5a128ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5015ba2090544f259f5503b33ac6bd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5167ce415f4f3091771b8bf13400a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {k: test(v) for k, v in test_loaders.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testin: 49.73 wer\n",
      "testnz: 14.72 wer\n",
      "  test: 14.85 wer\n",
      "   dev: 16.04 wer\n"
     ]
    }
   ],
   "source": [
    "for k, (wer, acc) in results.items():\n",
    "    print(f'{k:>6}: {wer:.2f} wer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
