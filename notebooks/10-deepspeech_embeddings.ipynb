{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeech with kaldi features and embedded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart from here\n",
    "DEV = True\n",
    "EPOCHS = 3\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from warpctc_pytorch import CTCLoss\n",
    "#torch.multiprocessing.set_start_method(\"spawn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "True# autoreloads\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport parameters\n",
    "\n",
    "# Allows to load modules from parent directory\n",
    "from time import time\n",
    "import inspect, sys, os, json\n",
    "from os.path import dirname, abspath\n",
    "sys.path.append(dirname(dirname(abspath(inspect.getfile(inspect.currentframe())))))\n",
    "\n",
    "from pathlib import Path\n",
    "from os import makedirs\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.data_loader import create_binarizer, get_accents_counts\n",
    "from utils import count_parameters\n",
    "from models.modules import MaskConv, SequenceWise, BatchRNN, InferenceBatchSoftmax, Lookahead, \\\n",
    "                    supported_rnns, supported_rnns_inv\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from decoder import GreedyDecoder, BeamCTCDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = parameters.get_parameters(dev=DEV, epochs=EPOCHS, us_en=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_cnts(list_):\n",
    "    return pd.Series(list_).value_counts()\n",
    "\n",
    "def extract_num (s):\n",
    "    return ''.join([c if c.isdigit() else '' for c in s])\n",
    "\n",
    "def ids_list(manifest):\n",
    "    ids = []\n",
    "    with open(manifest) as f:\n",
    "        for l in f:\n",
    "            s = l.split('/')\n",
    "            ids.append(f'{s[3]}-{s[5].split(\".\")[0]}')\n",
    "    return ids\n",
    "\n",
    "def make_accent_dict(manifest_path):\n",
    "    accent_dict = {}\n",
    "    class_dict = {}\n",
    "    with open(manifest_path) as f:\n",
    "        for l in f:\n",
    "            wav, txt, acc = l.split(',')\n",
    "            num = extract_num(wav)\n",
    "            accent = acc.strip()\n",
    "            if accent not in class_dict:\n",
    "                new_key = 0 if (len(class_dict) == 0) else max(class_dict.values()) + 1\n",
    "                class_dict[accent] = new_key\n",
    "            accent_dict[num] = class_dict[accent]\n",
    "    return accent_dict, {v: k for k, v in class_dict.items()}\n",
    "\n",
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n",
    "    if a.is_cuda:\n",
    "        order_index = order_index.cuda()\n",
    "    return torch.index_select(a, dim, order_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaldiDeepspeechDataset(Dataset):\n",
    "    \"\"\"Defines an iterator over the dataset. This class is intended to be used with PyTorch DataLoader\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, labels, sample_ids, transcripts_path,\n",
    "                 accent_id_dict,  embeddings_path, ivectors_path=None):\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.ivectors_path = ivectors_path\n",
    "        self.transcripts_path = transcripts_path\n",
    "        self.embeddings_path = embeddings_path\n",
    "        self.accent_id_dict = accent_id_dict\n",
    "        self.labels_map = dict([(labels[i], i) for i in range(len(labels))])\n",
    "        if isinstance(sample_ids, list):\n",
    "            self._datafiles = sample_ids\n",
    "        else:\n",
    "            with open(sample_ids) as f:\n",
    "                self._datafiles = [x.strip() for x in f.readlines()]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        file_idx = self._datafiles[index]\n",
    "        with open(os.path.join(self.data_path, file_idx)) as f:\n",
    "            sample = json.load(f)\n",
    "        sample = torch.FloatTensor(sample)\n",
    "        \n",
    "        target = self.accent_id_dict[extract_num(self._datafiles[index])]\n",
    "        \n",
    "        s_id = file_idx.split('-')[-1]\n",
    "\n",
    "        transcript_path = f'{self.transcripts_path}sample-{s_id}.txt'\n",
    "        transcript = self.parse_transcript(transcript_path)\n",
    "\n",
    "        try:\n",
    "            embedding = torch.load(f'{self.embeddings_path}{s_id}', map_location=lambda storage, loc: storage)\n",
    "        except Exception as e:\n",
    "            print(e, 'sample at fault:', self.embeddings_path, s_id)\n",
    "        \n",
    "        if self.ivectors_path is None:\n",
    "            return torch.FloatTensor(sample), target, embedding, transcript\n",
    "        else:\n",
    "            with open(os.path.join(self.ivectors_path, self._datafiles[index])) as f:\n",
    "                ivect = json.load(f)\n",
    "            return torch.FloatTensor(sample), target, transcript, embedding, torch.FloatTensor(ivect)\n",
    "        \n",
    "    def parse_transcript(self, transcript_path):\n",
    "        with open(transcript_path, 'r', encoding='utf8') as transcript_file:\n",
    "            transcript = transcript_file.read().replace('\\n', '')\n",
    "        transcript = list(filter(None, [self.labels_map.get(x) for x in list(transcript)]))\n",
    "        return transcript\n",
    "                      \n",
    "    def __len__(self):\n",
    "        return len(self._datafiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_tot):\n",
    "    \"\"\"This function takes list of samples and assembles a batch. It is intended to used in PyTorch DataLoader.\"\"\"\n",
    "    batch = list(zip(*batch_tot))\n",
    "    ivect = None\n",
    "    \n",
    "    if len(batch) == 4:\n",
    "        input_, acc, emb, trs = batch\n",
    "    elif len(batch) == 5:\n",
    "        input_, acc, emb, trs, ivect = batch\n",
    "\n",
    "    input_lens = torch.tensor([len(r) for r in input_])\n",
    "    acc = torch.tensor(acc)\n",
    "    \n",
    "    input_ = nn.utils.rnn.pad_sequence(input_, batch_first=True)\n",
    "\n",
    "    target_lens = torch.tensor([len(t) for t in trs])\n",
    "\n",
    "    if ivect is not None:\n",
    "        ivect = nn.utils.rnn.pad_sequence(ivect, batch_first=True)\n",
    "        ivect = tile(ivect, 1, 10)\n",
    "        ivect = ivect[:, :input_.size(1), :]\n",
    "        input_ = torch.cat([input_, ivect], dim=2)\n",
    "    \n",
    "    __, idx = input_lens.sort(descending=True)\n",
    "    \n",
    "    targets = np.array(trs)[idx]\n",
    "    targets = torch.tensor([t for target in targets for t in target])\n",
    "    \n",
    "    input_ = input_[idx]\n",
    "    input_lens = input_lens[idx].int()\n",
    "    targets = targets.int()\n",
    "    target_lens = target_lens[idx].int()\n",
    "    acc = acc[idx].int()\n",
    "\n",
    "    emb = torch.cat(emb)\n",
    "    emb = emb[idx]\n",
    "    emb = emb.view(emb.size(0), 1, emb.size(1))\n",
    "    emb = tile(emb, 1, input_.size(1))\n",
    "\n",
    "    input_ = torch.cat([input_, emb], dim=2)\n",
    "    \n",
    "    return input_, input_lens, targets, target_lens, acc\n",
    "\n",
    "class KaldiDeepspeechDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a data loader for SpeechDatasets.\n",
    "        \"\"\"\n",
    "        super(KaldiDeepspeechDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accent_id_dict, accent_dict = make_accent_dict(param['train_manifest'])\n",
    "\n",
    "train_dataset = KaldiDeepspeechDataset(data_path=param['train_kaldi'],\n",
    "                              labels=param['labels'],\n",
    "                              sample_ids=ids_list(param['train_manifest']), \n",
    "                              transcripts_path=param['train_transcripts'],\n",
    "                              embeddings_path=param['train_embeddings_256'],\n",
    "                              accent_id_dict=accent_id_dict,\n",
    "                              ivectors_path=None)\n",
    "\n",
    "train_loader = KaldiDeepspeechDataLoader(train_dataset, \n",
    "                                shuffle=True, \n",
    "                                num_workers=0,#param['num_worker'],\n",
    "                                batch_size=param['batch_size'])\n",
    "\n",
    "# for data in train_loader:    \n",
    "#     split_targets = []\n",
    "#     offset = 0\n",
    "#     for size in data[3]:\n",
    "#         split_targets.append(data[2][offset:offset + size])\n",
    "#         offset += size\n",
    "#     target_strings = decoder.convert_to_strings(split_targets)\n",
    "#     print('TARGETS', target_strings)\n",
    "#     break\n",
    "\n",
    "# for data in tqdm(train_loader):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict, __ = make_accent_dict(param['test_manifest'])\n",
    "\n",
    "test_dataset = KaldiDeepspeechDataset(data_path=param['test_kaldi'],\n",
    "                              labels=param['labels'],\n",
    "                              sample_ids=ids_list(param['test_manifest']), \n",
    "                              transcripts_path=param['test_transcripts'],\n",
    "                              embeddings_path=param['test_embeddings_256'],\n",
    "                              accent_id_dict=test_dict,\n",
    "                              ivectors_path=None)\n",
    "\n",
    "test_loader = KaldiDeepspeechDataLoader(test_dataset, \n",
    "                                shuffle=True, \n",
    "                                num_workers=param['num_worker'],\n",
    "                                batch_size=param['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSpeech(nn.Module):\n",
    "    def __init__(self, \n",
    "                rnn_type=nn.LSTM, \n",
    "                labels=\"abc\", \n",
    "                rnn_hidden_size=768, \n",
    "                nb_layers=5, \n",
    "                audio_conf=None,\n",
    "                bidirectional=True,\n",
    "                DEBUG=False):\n",
    "\n",
    "        super(DeepSpeech, self).__init__()\n",
    "\n",
    "        # model metadata needed for serialization/deserialization\n",
    "        if audio_conf is None:\n",
    "            audio_conf = {}\n",
    "        self._DEBUG = DEBUG\n",
    "        self._version = '0.0.1'\n",
    "        self._hidden_size = rnn_hidden_size\n",
    "        self._nb_layers = nb_layers\n",
    "        self._rnn_type = rnn_type\n",
    "        self._audio_conf = audio_conf or {}\n",
    "        self._labels = labels\n",
    "        self._bidirectional = bidirectional\n",
    "\n",
    "        sample_rate = self._audio_conf.get(\"sample_rate\", 16000)\n",
    "        window_size = self._audio_conf.get(\"window_size\", 0.02)\n",
    "        num_classes = len(self._labels)\n",
    "\n",
    "        self.conv = MaskConv(nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True)\n",
    "        ))\n",
    "\n",
    "        rnn_input_size = 1120\n",
    "\n",
    "        rnns = []\n",
    "        rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "                       bidirectional=bidirectional, batch_norm=False)\n",
    "        rnns.append(('0', rnn))\n",
    "        for x in range(nb_layers - 1):\n",
    "            rnn = BatchRNN(input_size=rnn_hidden_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "                           bidirectional=bidirectional)\n",
    "            rnns.append(('%d' % (x + 1), rnn))\n",
    "        self.rnns = nn.Sequential(OrderedDict(rnns))\n",
    "\n",
    "        fully_connected = nn.Sequential(\n",
    "            nn.BatchNorm1d(rnn_hidden_size),\n",
    "            nn.Linear(rnn_hidden_size, num_classes, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            SequenceWise(fully_connected),\n",
    "        )\n",
    "        self.inference_softmax = InferenceBatchSoftmax()\n",
    "\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        if self._DEBUG:\n",
    "            print('input', x.size())\n",
    "\n",
    "        lengths = lengths.cpu().int()\n",
    "        output_lengths = self.get_seq_lens(lengths)\n",
    "        \n",
    "        x = x.view(x.size(0), 1, x.size(1), x.size(2))\n",
    "        x = x.transpose(2, 3)\n",
    "        if self._DEBUG:\n",
    "            print('after view transpose', x.size())\n",
    "            \n",
    "        x, _ = self.conv(x, output_lengths)\n",
    "        if self._DEBUG:\n",
    "            print('after conv', x.size())\n",
    "\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Collapse feature dimension\n",
    "        x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
    "        if self._DEBUG:\n",
    "            print('after view transpose', x.size())\n",
    "\n",
    "        for rnn in self.rnns:\n",
    "            x = rnn(x, output_lengths)\n",
    "        if self._DEBUG:\n",
    "            print('after rnn', x.size())\n",
    "\n",
    "        x = self.fc(x)\n",
    "        if self._DEBUG:\n",
    "            print('after fc', x.size())\n",
    "        \n",
    "        x = x.transpose(0, 1)\n",
    "        if self._DEBUG:\n",
    "            print('after transpose', x.size())\n",
    "        # identity in training mode, softmax in eval mode\n",
    "        x = self.inference_softmax(x)\n",
    "        if self._DEBUG:\n",
    "            print('after softmax', x.size())\n",
    "            \n",
    "        x = x.transpose(0, 1)\n",
    "        if self._DEBUG:\n",
    "            print('after transpose', x.size())\n",
    "            \n",
    "        self._DEBUG = False\n",
    "        return x, output_lengths\n",
    "\n",
    "    def get_seq_lens(self, input_length):\n",
    "        \"\"\"\n",
    "        Given a 1D Tensor or Variable containing integer sequence lengths, return a 1D tensor or variable\n",
    "        containing the size sequences that will be output by the network.\n",
    "        :param input_length: 1D Tensor\n",
    "        :return: 1D Tensor scaled by model\n",
    "        \"\"\"\n",
    "        seq_len = input_length\n",
    "        for m in self.conv.modules():\n",
    "            if type(m) == nn.modules.conv.Conv2d:\n",
    "                seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n",
    "        return seq_len.int()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_labels(model):\n",
    "        return model.module._labels if model.is_parallel(model) else model._labels\n",
    "\n",
    "    @staticmethod\n",
    "    def get_param_size(model):\n",
    "        params = 0\n",
    "        for p in model.parameters():\n",
    "            tmp = 1\n",
    "            for x in p.size():\n",
    "                tmp *= x\n",
    "            params += tmp\n",
    "        return params\n",
    "\n",
    "    @staticmethod\n",
    "    def get_audio_conf(model):\n",
    "        return model.module._audio_conf if DeepSpeech.is_parallel(model) else model._audio_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSpeech(\n",
      "  (conv): MaskConv(\n",
      "    (seq_module): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hardtanh(min_val=0, max_val=20, inplace)\n",
      "      (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Hardtanh(min_val=0, max_val=20, inplace)\n",
      "    )\n",
      "  )\n",
      "  (rnns): Sequential(\n",
      "    (0): BatchRNN(\n",
      "      (rnn): GRU(1120, 800, bidirectional=True)\n",
      "    )\n",
      "    (1): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(800, 800, bidirectional=True)\n",
      "    )\n",
      "    (2): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(800, 800, bidirectional=True)\n",
      "    )\n",
      "    (3): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(800, 800, bidirectional=True)\n",
      "    )\n",
      "    (4): BatchRNN(\n",
      "      (batch_norm): SequenceWise (\n",
      "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
      "      (rnn): GRU(800, 800, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): SequenceWise (\n",
      "    Sequential(\n",
      "      (0): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Linear(in_features=800, out_features=29, bias=False)\n",
      "    ))\n",
      "  )\n",
      "  (inference_softmax): InferenceBatchSoftmax()\n",
      ")\n",
      "Model parameters counts: 40266368\n"
     ]
    }
   ],
   "source": [
    "model = DeepSpeech(rnn_type=param['rnn_type'], \n",
    "                labels=param['labels'], \n",
    "                rnn_hidden_size=param['rnn_hidden_size'], \n",
    "                nb_layers=param['num_layers'], #audio_conf=audio_conf,\n",
    "                bidirectional=True,\n",
    "                DEBUG=DEBUG,)\n",
    "\n",
    "if param['cuda']:\n",
    "    model.cuda()\n",
    "\n",
    "criterion = CTCLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=param['lr'][0])\n",
    "\n",
    "decoder = BeamCTCDecoder(param['labels'], lm_path=param['lm_path'],\n",
    "                        alpha=0.8, beta=1.,\n",
    "                        cutoff_top_n=40, cutoff_prob=1.0,\n",
    "                        beam_width=100, num_processes=param['num_worker'])\n",
    "target_decoder = GreedyDecoder(param['labels'])\n",
    "\n",
    "print(model)\n",
    "print('Model parameters counts:', count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wer(targets, targets_len, out, output_len):\n",
    "    split_targets = []\n",
    "    offset = 0\n",
    "    for size in targets_len:\n",
    "        split_targets.append(targets[offset:offset + size])\n",
    "        offset += size\n",
    "        \n",
    "    decoded_output, _ = decoder.decode(out.data.transpose(0,1), output_len)\n",
    "    target_strings = target_decoder.convert_to_strings(split_targets)\n",
    "    \n",
    "    if False:\n",
    "        print('targets', targets)\n",
    "        print('split_targets', split_targets)\n",
    "        print('out', out)\n",
    "        print('output_len', output_len)\n",
    "        print('decoded', decoded_output)\n",
    "        print('target', target_strings)\n",
    "    \n",
    "    wer, cer = 0, 0\n",
    "    for x in range(len(target_strings)):\n",
    "        transcript, reference = decoded_output[x][0], target_strings[x][0]\n",
    "        wer += decoder.wer(transcript, reference) / float(len(reference.split()))\n",
    "        #cer += decoder.cer(transcript, reference) / float(len(reference))\n",
    "    wer /= len(target_strings)\n",
    "    return wer * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, \n",
    "          model, \n",
    "          train_loader, \n",
    "          test_loader, \n",
    "          optimizer, \n",
    "          silent=True,\n",
    "          cnt=0,\n",
    "          exp_name='__tmp__'):\n",
    "\n",
    "    # Tensorboard\n",
    "    tb_path = Path(param['tensorboard_dir']) / exp_name\n",
    "    makedirs(tb_path, exist_ok=True)\n",
    "    tb_writer = SummaryWriter(tb_path)\n",
    "    best_model = model\n",
    "    \n",
    "    prev_epoch_val_loss = math.inf\n",
    "    prev_epoch_wer = math.inf\n",
    "    \n",
    "    ## Train\n",
    "    for epoch in range(1, param['epochs'] + 1):\n",
    "        import gc; gc.collect()\n",
    "        print('')\n",
    "        print(f'## EPOCH {epoch} ##')\n",
    "        print(f'Training:')\n",
    "        model.train()\n",
    "\n",
    "        # train\n",
    "        epoch_losses = []\n",
    "        for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            inputs, inputs_len, targets, targets_len, target_accents = data\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            inputs_len = inputs_len.cuda()\n",
    "            targets = targets.cuda()\n",
    "            targets_len = targets_len.cuda()\n",
    "            target_accents = target_accents.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            out, output_len = model(inputs, inputs_len)\n",
    "\n",
    "            out = out.cpu()\n",
    "            targets = targets.cpu()\n",
    "            targets_len = targets_len.cpu()\n",
    "            \n",
    "            if DEBUG:\n",
    "                print('## Outputs train')\n",
    "                print('out', out.size())\n",
    "                print('targets', targets.size())\n",
    "                print('output_len', output_len.size())\n",
    "                print('targets_len', targets_len.size())\n",
    "                   \n",
    "            loss = criterion(out, targets, output_len, targets_len)\n",
    "            epoch_losses.append(loss)\n",
    "\n",
    "            if not silent:\n",
    "                print(f'Iteration {i+1}/{len(train_loader):<4}loss: {loss.item():0.3f}')\n",
    "\n",
    "            # Gradient\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = sum(epoch_losses) / len(train_loader)\n",
    "        tb_writer.add_scalar('stats/train_loss', epoch_loss, epoch)\n",
    "        print(f'Epoch {epoch} average loss: {epoch_loss.item():0.3f}')\n",
    "\n",
    "        # validate\n",
    "        print(f'Testing:')\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        epoch_wer = []\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(test_loader, total=len(test_loader)): ## ## \n",
    "                inputs, inputs_len, targets, targets_len, target_accents = data\n",
    "                \n",
    "                inputs = inputs.cuda()\n",
    "                inputs_len = inputs_len.cuda()\n",
    "                targets = targets.cuda()\n",
    "                targets_len = targets_len.cuda()\n",
    "                target_accents = target_accents.cuda()\n",
    "\n",
    "                out, output_len = model(inputs, inputs_len)\n",
    "\n",
    "                out = out.cpu()\n",
    "                targets = targets.cpu()\n",
    "                targets_len = targets_len.cpu()\n",
    "                \n",
    "                if False:\n",
    "                    print('## Outputs test')\n",
    "                    print('out', out)\n",
    "                    print('targets', targets)\n",
    "                    print('output_len', output_len)\n",
    "                    print('targets_len', targets_len)\n",
    "                \n",
    "                val_loss = criterion(out, targets, output_len, targets_len)\n",
    "                \n",
    "                if DEBUG:\n",
    "                    print('val loss', val_loss)\n",
    "                \n",
    "                epoch_val_losses.append(val_loss)\n",
    "\n",
    "                wer = check_wer(targets, targets_len, out, output_len)\n",
    "                epoch_wer.append(wer)\n",
    "\n",
    "        epoch_val_loss = sum(epoch_val_losses) / len(epoch_val_losses) ##\n",
    "        epoch_wer = sum(epoch_wer) / len(epoch_wer)\n",
    "\n",
    "        tb_writer.add_scalar('stats/val_loss', epoch_val_loss, epoch)\n",
    "        print(f'Average validation loss: {epoch_val_loss.item():0.3f}')\n",
    "        \n",
    "        tb_writer.add_scalar('stats/wer', epoch_wer, epoch)\n",
    "        print(f'Average wer: {epoch_wer:0.3f}%')\n",
    "\n",
    "        if epoch_wer < prev_epoch_wer:\n",
    "            print('New best model found.')\n",
    "            prev_epoch_wer = epoch_wer     \n",
    "                \n",
    "            torch.save(model.state_dict, f'saved/vac05-ntbk_sd_{cnt}.pt')\n",
    "            torch.save(model, f'saved/vac05-ntbk_fm_{cnt}.pt')\n",
    "            \n",
    "    return model, prev_epoch_val_loss, prev_epoch_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "NEW-Embeddings_<class 'torch.nn.modules.rnn.GRU'>_hidden-800_1550754461.4420073\n",
      "#############\n",
      "\n",
      "## EPOCH 1 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c276607430425aac8cd65c2fcbac3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([20, 802, 140])\n",
      "after view transpose torch.Size([20, 1, 140, 802])\n",
      "after conv torch.Size([20, 32, 35, 401])\n",
      "after view transpose torch.Size([401, 20, 1120])\n",
      "after rnn torch.Size([401, 20, 800])\n",
      "after fc torch.Size([401, 20, 29])\n",
      "after transpose torch.Size([20, 401, 29])\n",
      "after softmax torch.Size([20, 401, 29])\n",
      "after transpose torch.Size([401, 20, 29])\n",
      "## Outputs train\n",
      "out torch.Size([401, 20, 29])\n",
      "targets torch.Size([1145])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([359, 20, 29])\n",
      "targets torch.Size([893])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([323, 20, 29])\n",
      "targets torch.Size([818])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([342, 20, 29])\n",
      "targets torch.Size([1078])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([316, 20, 29])\n",
      "targets torch.Size([973])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([413, 20, 29])\n",
      "targets torch.Size([1060])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([380, 20, 29])\n",
      "targets torch.Size([929])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([263, 20, 29])\n",
      "targets torch.Size([771])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([336, 20, 29])\n",
      "targets torch.Size([987])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([255, 20, 29])\n",
      "targets torch.Size([805])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([332, 20, 29])\n",
      "targets torch.Size([978])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([351, 20, 29])\n",
      "targets torch.Size([895])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([351, 20, 29])\n",
      "targets torch.Size([838])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([318, 20, 29])\n",
      "targets torch.Size([892])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([390, 20, 29])\n",
      "targets torch.Size([931])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([335, 20, 29])\n",
      "targets torch.Size([1038])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([327, 20, 29])\n",
      "targets torch.Size([997])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([305, 20, 29])\n",
      "targets torch.Size([900])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([375, 20, 29])\n",
      "targets torch.Size([921])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([398, 20, 29])\n",
      "targets torch.Size([1059])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([387, 20, 29])\n",
      "targets torch.Size([924])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([286, 20, 29])\n",
      "targets torch.Size([793])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([387, 20, 29])\n",
      "targets torch.Size([912])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([353, 20, 29])\n",
      "targets torch.Size([829])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([374, 20, 29])\n",
      "targets torch.Size([852])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([359, 20, 29])\n",
      "targets torch.Size([821])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([322, 20, 29])\n",
      "targets torch.Size([939])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([346, 20, 29])\n",
      "targets torch.Size([1034])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([369, 20, 29])\n",
      "targets torch.Size([974])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([428, 20, 29])\n",
      "targets torch.Size([822])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([351, 20, 29])\n",
      "targets torch.Size([1067])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([345, 20, 29])\n",
      "targets torch.Size([728])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([389, 20, 29])\n",
      "targets torch.Size([969])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([363, 20, 29])\n",
      "targets torch.Size([897])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([476, 20, 29])\n",
      "targets torch.Size([794])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([280, 20, 29])\n",
      "targets torch.Size([906])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([260, 20, 29])\n",
      "targets torch.Size([798])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([392, 20, 29])\n",
      "targets torch.Size([893])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([372, 20, 29])\n",
      "targets torch.Size([1070])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([422, 20, 29])\n",
      "targets torch.Size([951])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([311, 20, 29])\n",
      "targets torch.Size([1021])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([353, 20, 29])\n",
      "targets torch.Size([798])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([342, 20, 29])\n",
      "targets torch.Size([852])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([311, 20, 29])\n",
      "targets torch.Size([933])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([456, 20, 29])\n",
      "targets torch.Size([932])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([282, 20, 29])\n",
      "targets torch.Size([1026])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([377, 20, 29])\n",
      "targets torch.Size([989])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([305, 20, 29])\n",
      "targets torch.Size([770])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([293, 20, 29])\n",
      "targets torch.Size([972])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([377, 20, 29])\n",
      "targets torch.Size([829])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([344, 20, 29])\n",
      "targets torch.Size([1004])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([428, 20, 29])\n",
      "targets torch.Size([1048])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([480, 20, 29])\n",
      "targets torch.Size([1058])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([441, 20, 29])\n",
      "targets torch.Size([1044])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([350, 20, 29])\n",
      "targets torch.Size([1082])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([419, 20, 29])\n",
      "targets torch.Size([960])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([309, 20, 29])\n",
      "targets torch.Size([840])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([197, 2, 29])\n",
      "targets torch.Size([83])\n",
      "output_len torch.Size([2])\n",
      "targets_len torch.Size([2])\n",
      "\n",
      "Epoch 1 average loss: 3255.473\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dccb409fc74c329af9b30395cb2463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor([9251.6553])\n",
      "val loss tensor([10398.5449])\n",
      "val loss tensor([7921.9023])\n",
      "val loss tensor([8488.9023])\n",
      "val loss tensor([8875.7080])\n",
      "val loss tensor([9150.7471])\n",
      "val loss tensor([8625.9453])\n",
      "val loss tensor([8629.1025])\n",
      "val loss tensor([7481.4575])\n",
      "val loss tensor([9340.7461])\n",
      "val loss tensor([10260.9707])\n",
      "val loss tensor([8796.2314])\n",
      "val loss tensor([9223.7275])\n",
      "val loss tensor([8891.9434])\n",
      "val loss tensor([8810.9385])\n",
      "val loss tensor([9466.3682])\n",
      "val loss tensor([7946.8330])\n",
      "val loss tensor([8669.7842])\n",
      "val loss tensor([9182.7432])\n",
      "val loss tensor([10589.4014])\n",
      "val loss tensor([9414.7119])\n",
      "val loss tensor([9544.4336])\n",
      "val loss tensor([7565.1987])\n",
      "val loss tensor([8215.7236])\n",
      "val loss tensor([7410.2422])\n",
      "val loss tensor([8056.8535])\n",
      "val loss tensor([10706.9346])\n",
      "val loss tensor([8810.3184])\n",
      "val loss tensor([9367.6289])\n",
      "val loss tensor([9232.0752])\n",
      "val loss tensor([10191.5840])\n",
      "val loss tensor([10104.2773])\n",
      "val loss tensor([8242.9902])\n",
      "val loss tensor([8539.8525])\n",
      "val loss tensor([9251.1162])\n",
      "val loss tensor([9792.5527])\n",
      "val loss tensor([8528.0293])\n",
      "val loss tensor([8171.7300])\n",
      "val loss tensor([9063.7920])\n",
      "val loss tensor([8812.9971])\n",
      "val loss tensor([9040.6445])\n",
      "val loss tensor([8102.8076])\n",
      "val loss tensor([10558.9629])\n",
      "val loss tensor([9578.7354])\n",
      "val loss tensor([10876.9150])\n",
      "val loss tensor([9234.5742])\n",
      "val loss tensor([9513.8896])\n",
      "val loss tensor([8258.9951])\n",
      "val loss tensor([10466.8652])\n",
      "val loss tensor([8318.0479])\n",
      "val loss tensor([8491.8447])\n",
      "val loss tensor([8639.5625])\n",
      "val loss tensor([9350.1396])\n",
      "val loss tensor([9557.6006])\n",
      "val loss tensor([8407.1133])\n",
      "val loss tensor([8980.7705])\n",
      "val loss tensor([3229.2793])\n",
      "\n",
      "Average validation loss: 8940.939\n",
      "Average wer: 96.487%\n",
      "New best model found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type DeepSpeech. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## EPOCH 2 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32156795ded24bdcbc32e1016b02b1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Outputs train\n",
      "out torch.Size([476, 20, 29])\n",
      "targets torch.Size([1130])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([366, 20, 29])\n",
      "targets torch.Size([1089])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([353, 20, 29])\n",
      "targets torch.Size([1024])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([327, 20, 29])\n",
      "targets torch.Size([1059])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([350, 20, 29])\n",
      "targets torch.Size([909])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([419, 20, 29])\n",
      "targets torch.Size([957])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([298, 20, 29])\n",
      "targets torch.Size([882])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([333, 20, 29])\n",
      "targets torch.Size([980])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([342, 20, 29])\n",
      "targets torch.Size([879])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([401, 20, 29])\n",
      "targets torch.Size([898])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([413, 20, 29])\n",
      "targets torch.Size([809])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([305, 20, 29])\n",
      "targets torch.Size([847])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([318, 20, 29])\n",
      "targets torch.Size([736])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([351, 20, 29])\n",
      "targets torch.Size([853])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([311, 20, 29])\n",
      "targets torch.Size([892])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([322, 20, 29])\n",
      "targets torch.Size([959])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([351, 20, 29])\n",
      "targets torch.Size([898])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([377, 20, 29])\n",
      "targets torch.Size([955])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([344, 20, 29])\n",
      "targets torch.Size([1000])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([372, 20, 29])\n",
      "targets torch.Size([1047])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([387, 20, 29])\n",
      "targets torch.Size([908])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([480, 20, 29])\n",
      "targets torch.Size([897])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([318, 20, 29])\n",
      "targets torch.Size([872])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([309, 20, 29])\n",
      "targets torch.Size([799])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([374, 20, 29])\n",
      "targets torch.Size([908])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([345, 20, 29])\n",
      "targets torch.Size([842])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([350, 20, 29])\n",
      "targets torch.Size([856])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([317, 20, 29])\n",
      "targets torch.Size([774])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([318, 20, 29])\n",
      "targets torch.Size([859])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([381, 20, 29])\n",
      "targets torch.Size([1128])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([353, 20, 29])\n",
      "targets torch.Size([932])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([310, 20, 29])\n",
      "targets torch.Size([974])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([390, 20, 29])\n",
      "targets torch.Size([855])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([456, 20, 29])\n",
      "targets torch.Size([1053])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([441, 20, 29])\n",
      "targets torch.Size([946])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([422, 20, 29])\n",
      "targets torch.Size([905])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([398, 20, 29])\n",
      "targets torch.Size([954])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([332, 20, 29])\n",
      "targets torch.Size([956])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([389, 20, 29])\n",
      "targets torch.Size([921])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([297, 20, 29])\n",
      "targets torch.Size([877])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([323, 20, 29])\n",
      "targets torch.Size([945])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([395, 20, 29])\n",
      "targets torch.Size([970])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([428, 20, 29])\n",
      "targets torch.Size([932])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([347, 20, 29])\n",
      "targets torch.Size([832])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([399, 20, 29])\n",
      "targets torch.Size([890])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([387, 20, 29])\n",
      "targets torch.Size([1153])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([380, 20, 29])\n",
      "targets torch.Size([804])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([299, 20, 29])\n",
      "targets torch.Size([1038])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([334, 20, 29])\n",
      "targets torch.Size([952])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([261, 20, 29])\n",
      "targets torch.Size([862])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([324, 20, 29])\n",
      "targets torch.Size([952])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([339, 20, 29])\n",
      "targets torch.Size([800])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([311, 20, 29])\n",
      "targets torch.Size([935])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([315, 20, 29])\n",
      "targets torch.Size([1097])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([388, 20, 29])\n",
      "targets torch.Size([1256])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([377, 20, 29])\n",
      "targets torch.Size([855])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([339, 20, 29])\n",
      "targets torch.Size([735])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([203, 2, 29])\n",
      "targets torch.Size([95])\n",
      "output_len torch.Size([2])\n",
      "targets_len torch.Size([2])\n",
      "\n",
      "Epoch 2 average loss: 2677.375\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a175be64b2eb4ce1b77afe57064a423c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor([8244.7842])\n",
      "val loss tensor([10126.4688])\n",
      "val loss tensor([8612.9512])\n",
      "val loss tensor([10285.3887])\n",
      "val loss tensor([9904.1318])\n",
      "val loss tensor([9013.9658])\n",
      "val loss tensor([8580.0664])\n",
      "val loss tensor([9090.2520])\n",
      "val loss tensor([7033.3867])\n",
      "val loss tensor([9030.8906])\n",
      "val loss tensor([9058.4219])\n",
      "val loss tensor([8893.6377])\n",
      "val loss tensor([8632.1523])\n",
      "val loss tensor([9035.9697])\n",
      "val loss tensor([8973.3574])\n",
      "val loss tensor([10336.5283])\n",
      "val loss tensor([8635.1924])\n",
      "val loss tensor([9545.7158])\n",
      "val loss tensor([8772.7812])\n",
      "val loss tensor([9507.0801])\n",
      "val loss tensor([9028.8330])\n",
      "val loss tensor([8685.9385])\n",
      "val loss tensor([9972.3682])\n",
      "val loss tensor([8371.1143])\n",
      "val loss tensor([8472.6318])\n",
      "val loss tensor([9561.3438])\n",
      "val loss tensor([8300.1953])\n",
      "val loss tensor([8756.1543])\n",
      "val loss tensor([8799.3271])\n",
      "val loss tensor([9480.7646])\n",
      "val loss tensor([10936.5439])\n",
      "val loss tensor([8879.8428])\n",
      "val loss tensor([8709.4971])\n",
      "val loss tensor([8887.0879])\n",
      "val loss tensor([8852.4990])\n",
      "val loss tensor([9784.4199])\n",
      "val loss tensor([9246.2031])\n",
      "val loss tensor([10590.0605])\n",
      "val loss tensor([10345.1963])\n",
      "val loss tensor([9043.9238])\n",
      "val loss tensor([9728.0605])\n",
      "val loss tensor([9976.6670])\n",
      "val loss tensor([9976.6152])\n",
      "val loss tensor([10229.5322])\n",
      "val loss tensor([6550.9902])\n",
      "val loss tensor([8945.5010])\n",
      "val loss tensor([8689.8906])\n",
      "val loss tensor([8527.5645])\n",
      "val loss tensor([9046.8965])\n",
      "val loss tensor([8791.7510])\n",
      "val loss tensor([9034.3965])\n",
      "val loss tensor([8814.7246])\n",
      "val loss tensor([8519.5459])\n",
      "val loss tensor([9022.7002])\n",
      "val loss tensor([9393.9502])\n",
      "val loss tensor([9667.6191])\n",
      "val loss tensor([3166.9199])\n",
      "\n",
      "Average validation loss: 9019.305\n",
      "Average wer: 94.458%\n",
      "New best model found.\n",
      "\n",
      "## EPOCH 3 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee26c86784442e88fff3d2c9e9e02d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Outputs train\n",
      "out torch.Size([369, 20, 29])\n",
      "targets torch.Size([909])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([286, 20, 29])\n",
      "targets torch.Size([920])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([316, 20, 29])\n",
      "targets torch.Size([1061])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([357, 20, 29])\n",
      "targets torch.Size([1054])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([387, 20, 29])\n",
      "targets torch.Size([1126])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([328, 20, 29])\n",
      "targets torch.Size([823])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([387, 20, 29])\n",
      "targets torch.Size([985])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([279, 20, 29])\n",
      "targets torch.Size([652])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([329, 20, 29])\n",
      "targets torch.Size([678])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([344, 20, 29])\n",
      "targets torch.Size([1012])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([333, 20, 29])\n",
      "targets torch.Size([822])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([441, 20, 29])\n",
      "targets torch.Size([909])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([298, 20, 29])\n",
      "targets torch.Size([831])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([377, 20, 29])\n",
      "targets torch.Size([1051])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([327, 20, 29])\n",
      "targets torch.Size([981])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([398, 20, 29])\n",
      "targets torch.Size([880])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([389, 20, 29])\n",
      "targets torch.Size([976])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([428, 20, 29])\n",
      "targets torch.Size([980])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([347, 20, 29])\n",
      "targets torch.Size([910])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([419, 20, 29])\n",
      "targets torch.Size([935])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([305, 20, 29])\n",
      "targets torch.Size([914])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([352, 20, 29])\n",
      "targets torch.Size([984])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([480, 20, 29])\n",
      "targets torch.Size([764])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([428, 20, 29])\n",
      "targets torch.Size([987])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([456, 20, 29])\n",
      "targets torch.Size([1007])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([318, 20, 29])\n",
      "targets torch.Size([906])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([339, 20, 29])\n",
      "targets torch.Size([829])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([316, 20, 29])\n",
      "targets torch.Size([917])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([374, 20, 29])\n",
      "targets torch.Size([941])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([377, 20, 29])\n",
      "targets torch.Size([935])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([317, 20, 29])\n",
      "targets torch.Size([982])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([305, 20, 29])\n",
      "targets torch.Size([972])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([298, 20, 29])\n",
      "targets torch.Size([837])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([476, 20, 29])\n",
      "targets torch.Size([1098])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([346, 20, 29])\n",
      "targets torch.Size([1015])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([399, 20, 29])\n",
      "targets torch.Size([1011])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([377, 20, 29])\n",
      "targets torch.Size([669])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([353, 20, 29])\n",
      "targets torch.Size([960])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([353, 20, 29])\n",
      "targets torch.Size([1060])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([366, 20, 29])\n",
      "targets torch.Size([1010])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([395, 20, 29])\n",
      "targets torch.Size([899])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([413, 20, 29])\n",
      "targets torch.Size([852])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([329, 20, 29])\n",
      "targets torch.Size([930])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([359, 20, 29])\n",
      "targets torch.Size([943])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([369, 20, 29])\n",
      "targets torch.Size([1012])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([422, 20, 29])\n",
      "targets torch.Size([971])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([359, 20, 29])\n",
      "targets torch.Size([866])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([392, 20, 29])\n",
      "targets torch.Size([1022])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([372, 20, 29])\n",
      "targets torch.Size([886])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([333, 20, 29])\n",
      "targets torch.Size([916])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([390, 20, 29])\n",
      "targets torch.Size([905])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([285, 20, 29])\n",
      "targets torch.Size([886])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([311, 20, 29])\n",
      "targets torch.Size([882])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([290, 20, 29])\n",
      "targets torch.Size([958])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([311, 20, 29])\n",
      "targets torch.Size([988])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([251, 20, 29])\n",
      "targets torch.Size([847])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([388, 20, 29])\n",
      "targets torch.Size([996])\n",
      "output_len torch.Size([20])\n",
      "targets_len torch.Size([20])\n",
      "## Outputs train\n",
      "out torch.Size([136, 2, 29])\n",
      "targets torch.Size([70])\n",
      "output_len torch.Size([2])\n",
      "targets_len torch.Size([2])\n",
      "\n",
      "Epoch 3 average loss: 2581.918\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7cd7b2a37b4332abde701f86f22850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor([10427.1611])\n",
      "val loss tensor([7871.2642])\n",
      "val loss tensor([8618.6748])\n",
      "val loss tensor([8771.4727])\n",
      "val loss tensor([9047.2969])\n",
      "val loss tensor([9007.6074])\n",
      "val loss tensor([8250.8740])\n",
      "val loss tensor([8449.5234])\n",
      "val loss tensor([10042.9766])\n",
      "val loss tensor([8764.8389])\n",
      "val loss tensor([7537.1787])\n",
      "val loss tensor([8023.8892])\n",
      "val loss tensor([9108.5947])\n",
      "val loss tensor([8168.7554])\n",
      "val loss tensor([10554.2861])\n",
      "val loss tensor([9269.7617])\n",
      "val loss tensor([9446.2246])\n",
      "val loss tensor([7896.7500])\n",
      "val loss tensor([9291.1094])\n",
      "val loss tensor([8663.5605])\n",
      "val loss tensor([8988.1650])\n",
      "val loss tensor([9210.6064])\n",
      "val loss tensor([8521.4316])\n",
      "val loss tensor([9507.9697])\n",
      "val loss tensor([9170.6670])\n",
      "val loss tensor([8330.6826])\n",
      "val loss tensor([8386.0508])\n",
      "val loss tensor([8735.6631])\n",
      "val loss tensor([10634.6016])\n",
      "val loss tensor([10114.1318])\n",
      "val loss tensor([9038.6113])\n",
      "val loss tensor([8662.1260])\n",
      "val loss tensor([8626.0928])\n",
      "val loss tensor([8911.4561])\n",
      "val loss tensor([9430.7168])\n",
      "val loss tensor([10633.5293])\n",
      "val loss tensor([8610.8682])\n",
      "val loss tensor([8608.4482])\n",
      "val loss tensor([9093.6885])\n",
      "val loss tensor([9586.7119])\n",
      "val loss tensor([7842.5752])\n",
      "val loss tensor([9742.8389])\n",
      "val loss tensor([8224.9248])\n",
      "val loss tensor([8061.2808])\n",
      "val loss tensor([9732.2451])\n",
      "val loss tensor([9636.7324])\n",
      "val loss tensor([8186.2490])\n",
      "val loss tensor([8900.2793])\n",
      "val loss tensor([8715.9980])\n",
      "val loss tensor([9084.6201])\n",
      "val loss tensor([9166.3467])\n",
      "val loss tensor([9602.6123])\n",
      "val loss tensor([8374.4648])\n",
      "val loss tensor([10061.4170])\n",
      "val loss tensor([8610.2080])\n",
      "val loss tensor([7979.7188])\n",
      "val loss tensor([3215.7134])\n",
      "\n",
      "Average validation loss: 8862.321\n",
      "Average wer: 95.307%\n"
     ]
    }
   ],
   "source": [
    "settings = {'rnn_type': [nn.GRU],\n",
    "            'rnn_hidden_size': [800],}\n",
    "i = 0\n",
    "for _rnn_type in settings['rnn_type']:\n",
    "    for _rnn_hidden_size in settings['rnn_hidden_size']:\n",
    "        exp_name = f'NEW-Embeddings_{_rnn_type}_hidden-{_rnn_hidden_size}_{time()}'\n",
    "        i += 1\n",
    "        model = DeepSpeech(rnn_type=_rnn_type, \n",
    "                        labels=param['labels'], \n",
    "                        rnn_hidden_size=_rnn_hidden_size, \n",
    "                        nb_layers=param['num_layers'], #audio_conf=audio_conf,\n",
    "                        bidirectional=True,\n",
    "                        DEBUG=DEBUG,)\n",
    "\n",
    "        if param['cuda']:\n",
    "            model.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=param['lr'][0])\n",
    "\n",
    "        print()\n",
    "        print(f'{\"\":#<13}')\n",
    "        print(exp_name)\n",
    "        print(f'{\"\":#<13}')\n",
    "\n",
    "        model, val_loss, wer = train(param, \n",
    "                                model,\n",
    "                                train_loader, \n",
    "                                test_loader, optimizer, \n",
    "                                cnt=i,\n",
    "                                exp_name=exp_name)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=2)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"black\") #if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = test_loader\n",
    "best_model= model\n",
    "best_model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(loader, total=len(loader)):\n",
    "        inputs, lens, targets, target_lens, target_accents = data\n",
    "        inputs = inputs.cuda()\n",
    "        target_accents = target_accents.cuda()\n",
    "        \n",
    "        out, __ = best_model(inputs, lens)\n",
    "        \n",
    "        y_true.extend(target_accents)\n",
    "        y_pred.append(np.argmax(out, axis=1))\n",
    "        \n",
    "    y_pred = torch.cat(y_pred)\n",
    "            \n",
    "    y_true_labels = [accent_dict[int(i)] for i in y_true]\n",
    "    y_pred_labels = [accent_dict[int(i)] for i in y_pred]\n",
    "\n",
    "cnf_mat = confusion_matrix(y_true_labels, y_pred_labels, labels=list(accent_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnf_mat, classes=accent_dict.values(), normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_pca(X, y, _dict, projection='PCA', graph_title=''):\n",
    "    if projection == 'PCA':\n",
    "        Y = sklearnPCA(n_components=2).fit_transform(X)\n",
    "    elif projection == 'TSNE':\n",
    "        Y = TSNE(n_components=2).fit_transform(X)\n",
    "    else:\n",
    "        raise ValueError(f'Projection {projection} unkown.')\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for lab in _dict.values():\n",
    "        plt.scatter(Y[y==lab, 0],\n",
    "                    Y[y==lab, 1],\n",
    "                    label=lab)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.tight_layout()\n",
    "    plt.title(f'{projection}: {graph_title}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = test_loader\n",
    "projection = 'PCA'\n",
    "\n",
    "for model_name, (model, val_loss) in best_models.items():\n",
    "    datapoints = []\n",
    "    targets = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, total=len(loader)):\n",
    "            \n",
    "            inputs, target_accents, lens = data\n",
    "            inputs = inputs.cuda()\n",
    "            __, bn = best_model(inputs, lens)\n",
    "\n",
    "            datapoints.append(bn)\n",
    "            targets.append(target_accents)\n",
    "            \n",
    "    datapoints = torch.cat(datapoints)\n",
    "    targets = torch.cat(targets)\n",
    "    \n",
    "    X = np.asarray(datapoints)\n",
    "    y = np.asarray([accent_dict[t.item()] for t in targets])\n",
    "    \n",
    "    plot_pca(X, y, accent_dict, projection=projection, graph_title=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
