{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Accent classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = True\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreloads\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport parameters\n",
    "\n",
    "# Allows to load modules from parent directory\n",
    "from time import time\n",
    "import inspect, sys\n",
    "from os.path import dirname, abspath\n",
    "sys.path.append(dirname(dirname(abspath(inspect.getfile(inspect.currentframe())))))\n",
    "\n",
    "from pathlib import Path\n",
    "from os import makedirs\n",
    "\n",
    "#from models.accent_classifier import AccentClassifier\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from torch.nn.modules import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from data.data_loader import create_binarizer, get_accents_counts\n",
    "from data.data_loader import SpectrogramAccentDataset, BucketingSampler, AudioDataLoader\n",
    "from utils import count_parameters\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models.modules import MaskConv, SequenceWise, BatchRNN, InferenceBatchSoftmax, \\\n",
    "                    supported_rnns, supported_rnns_inv\n",
    "\n",
    "class AccentClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 labels,\n",
    "                 audio_conf={}, \n",
    "                 rnn_hidden_size=800,\n",
    "                 rnn_type=nn.GRU,\n",
    "                 DEBUG = False):\n",
    "        \n",
    "        super(AccentClassifier, self).__init__()\n",
    "        \n",
    "        self._DEBUG = DEBUG\n",
    "\n",
    "        # metadata\n",
    "        self._audio_conf = audio_conf\n",
    "        self._labels = labels\n",
    "        self._num_classes = len(labels)\n",
    "        \n",
    "        sample_rate = self._audio_conf.get(\"sample_rate\", 16000)\n",
    "        window_size = self._audio_conf.get(\"window_size\", 0.02)\n",
    "\n",
    "        self.conv = MaskConv(nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Hardtanh(0, 20, inplace=True)\n",
    "        ))\n",
    "\n",
    "        # Based on above convolutions and spectrogram size using conv formula (W - F + 2P)/ S+1\n",
    "        conv_output_size = int(math.floor((sample_rate * window_size) / 2) + 1)\n",
    "        conv_output_size = int(math.floor(conv_output_size + 2 * 20 - 41) / 2 + 1)\n",
    "        conv_output_size = int(math.floor(conv_output_size + 2 * 10 - 21) / 2 + 1)\n",
    "        conv_output_size *= 32\n",
    "\n",
    "        self.rnn = rnn_type(conv_output_size, conv_output_size, 1)\n",
    "        #self.rnn = BatchRNN(input_size=conv_output_size, \n",
    "        #                    hidden_size=conv_output_size, \n",
    "        #                    rnn_type=rnn_type, \n",
    "        #                    bidirectional=True, \n",
    "        #                    batch_norm=False)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(conv_output_size),\n",
    "            nn.Linear(conv_output_size, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, self._num_classes, bias=False),\n",
    "        )\n",
    "        \n",
    "        #self.fc = SequenceWise(fully_co)\n",
    "        \n",
    "        self.inference_softmax = InferenceBatchSoftmax()\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        lengths = lengths.cpu().int()\n",
    "        output_lengths = self.get_seq_lens(lengths)\n",
    "        if self._DEBUG:\n",
    "            print('input', x.size())\n",
    "        x, _ = self.conv(x, output_lengths)\n",
    "        if self._DEBUG:\n",
    "            print('afetr conv', x.size())\n",
    "        #sizes = x.size()\n",
    "        #x = x.view(sizes[0], sizes[1] * sizes[2] * sizes[3])  # Collapse feature dimension\n",
    "        #x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
    "        #print('bef', x.size())\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        #if DEBUG:\n",
    "            #print('after view', x.size())\n",
    "        #x = torch.sum(x, dim=2)\n",
    "        #if DEBUG:\n",
    "           # print('after sum', x.size())\n",
    "        #x = x.transpose(0, 1).contiguous()\n",
    "        \n",
    "        \n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2), x.size(3))\n",
    "        x = x.transpose(1, 2)\n",
    "        #x = x.transpose(0, 1)\n",
    "        \n",
    "        if self._DEBUG:\n",
    "            print('after view trans', x.size())\n",
    "        \n",
    "        x, __ = self.rnn(x)\n",
    "        \n",
    "        if self._DEBUG:\n",
    "            print('after gru', x.size())\n",
    "        \n",
    "        #x = x.transpose(0, 1)\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        if self._DEBUG:\n",
    "            print('after select', x.size())\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if self._DEBUG:\n",
    "            print('after fully co', x.size())\n",
    "        #x = x.transpose(0, 1)\n",
    "        if self._DEBUG:\n",
    "            print('after transpose', x.size())\n",
    "        # identity in training mode, softmax in eval mode\n",
    "        x = self.inference_softmax(x)\n",
    "        if self._DEBUG:\n",
    "            print('after inference', x.size())\n",
    "        #x = x.transpose(0, 1)\n",
    "        if self._DEBUG:\n",
    "            print('output', x.size())\n",
    "            print('###########')\n",
    "        return x\n",
    "    \n",
    "    def get_seq_lens(self, input_length):\n",
    "        \"\"\"\n",
    "        Given a 1D Tensor or Variable containing integer sequence lengths, return a 1D tensor or variable\n",
    "        containing the size sequences that will be output by the network.\n",
    "        :param input_length: 1D Tensor\n",
    "        :return: 1D Tensor scaled by model\n",
    "        \"\"\"\n",
    "        seq_len = input_length\n",
    "        for m in self.conv.modules():\n",
    "            if type(m) == nn.modules.conv.Conv2d:\n",
    "                seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n",
    "        return seq_len.int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiments by directly changing the values in parameters.py\n",
    "param = parameters.get_parameters(dev=DEV, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "exp_name = f'__tmp__accent_classfication_notebook_martigny_{time()}'\n",
    "tb_path = Path(param['tensorboard_dir']) / exp_name\n",
    "makedirs(tb_path, exist_ok=True)\n",
    "tb_writer = SummaryWriter(tb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    inputs, targets, input_percentages, target_sizes, target_accents = data\n",
    "    if len(target_accents[0]) > 1:\n",
    "        target_accents = np.argmax(target_accents, axis=1)\n",
    "    else:\n",
    "        target_accents = target_accents.view(target_accents.size(0))\n",
    "\n",
    "    if param['cuda']:\n",
    "        inputs = inputs.cuda()\n",
    "        target_accents = target_accents.cuda()\n",
    "        \n",
    "    input_sizes = input_percentages.mul_(int(inputs.size(3))).int()\n",
    "                \n",
    "    return inputs, target_accents, input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_conf = {'sample_rate': param['sample_rate'],\n",
    "                'window_size': param['window_size'],\n",
    "                'window_stride': param['window_stride'],\n",
    "                'window': param['window'],\n",
    "                'noise_dir': param['noise_dir'],\n",
    "                'noise_prob': param['noise_prob'],\n",
    "                'noise_levels': (param['noise_min'], param['noise_max'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accent_binarizer = create_binarizer(param['train_manifest'])\n",
    "labels = accent_binarizer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpectrogramAccentDataset(audio_conf=audio_conf, \n",
    "                                        manifest_filepath=param['train_manifest'], \n",
    "                                        labels=labels,\n",
    "                                        normalize=True, \n",
    "                                        augment=param['augment'], \n",
    "                                        accent_binarizer=accent_binarizer,\n",
    "                                        kaldi=False)\n",
    "\n",
    "train_sampler = BucketingSampler(train_dataset, batch_size=param['batch_size'])\n",
    "\n",
    "train_loader = AudioDataLoader(train_dataset,\n",
    "                                num_workers=param['num_worker'], \n",
    "                                batch_sampler=train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SpectrogramAccentDataset(audio_conf=audio_conf, \n",
    "                                        manifest_filepath=param['test_manifest'], \n",
    "                                        labels=labels,\n",
    "                                        normalize=True, \n",
    "                                        augment=False, \n",
    "                                        accent_binarizer=accent_binarizer,\n",
    "                                        kaldi=False)\n",
    "\n",
    "test_loader = AudioDataLoader(test_dataset,\n",
    "                                num_workers=param['num_worker'], \n",
    "                                batch_size=param['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%aimport` not found.\n"
     ]
    }
   ],
   "source": [
    "%aimport kaldi_io.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/thibault/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/thibault/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/thibault/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/thibault/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    }
   ],
   "source": [
    "create_data_folder(feats_path='../data/CommonVoice_dataset/kaldi/dev-norm.ark', \n",
    "                   out_path='../data/CommonVoice_dataset/kaldi/tmp2',\n",
    "                   mat_type='ark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_folder(feats_path='../data/CommonVoice_dataset/kaldi/ivectors_valid_dev_hires/ivector_online.10.scp', \n",
    "                   out_path='../data/CommonVoice_dataset/kaldi/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = ['cv-valid-dev-sample-001864_40', 'cv-valid-dev-sample-001951_24']\n",
    "train_dataset = SpeechDataset(data_path=param['train_kaldi'] + 'tmp', sample_ids=test_list)\n",
    "train_loader = SpeechDataLoader(train_dataset,\n",
    "                                num_workers=param['num_worker'], \n",
    "                                batch_size=param['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 100])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 40, 100])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = get_accents_counts(param['train_manifest'])\n",
    "class_counts = [train_counts[c] / max(train_counts.values()) for c in accent_binarizer.classes_]\n",
    "weights = 1 / torch.tensor(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccentClassifier(\n",
      "  (conv): MaskConv(\n",
      "    (seq_module): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hardtanh(min_val=0, max_val=20, inplace)\n",
      "      (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Hardtanh(min_val=0, max_val=20, inplace)\n",
      "    )\n",
      "  )\n",
      "  (rnn): GRU(1312, 1312)\n",
      "  (fc): Sequential(\n",
      "    (0): BatchNorm1d(1312, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Linear(in_features=1312, out_features=256, bias=False)\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=256, out_features=7, bias=False)\n",
      "  )\n",
      "  (inference_softmax): InferenceBatchSoftmax()\n",
      ")\n",
      "Model parameters counts: 10927904\n"
     ]
    }
   ],
   "source": [
    "model = AccentClassifier(labels=labels, \n",
    "                         audio_conf=audio_conf, \n",
    "                         rnn_hidden_size=param['rnn_hidden_size'],  \n",
    "                         rnn_type=param['rnn_type'], \n",
    "                         DEBUG=False)\n",
    "if param['cuda']:\n",
    "    model.cuda()\n",
    "    weights = weights.cuda()\n",
    "    \n",
    "criterion = CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=param['lr'][0])\n",
    "\n",
    "print(model)\n",
    "print('Model parameters counts:', count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## EPOCH 1 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20eca05c74a84029a1147f1d92ba38db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 average loss: 0.638\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a083093f3e374cf8936e4580a5291475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accent classification accuracy: 32.08%\n",
      "Average validation loss: 0.689\n",
      "New best model found.\n",
      "## EPOCH 2 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99968b6f6c24bcd8274645e234ef118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 average loss: 0.675\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f66bc091a24fcbbf27c0b0c626c458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accent classification accuracy: 32.08%\n",
      "Average validation loss: 0.687\n",
      "## EPOCH 3 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dd94c4e3d846798588bba3ab9ce7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 average loss: 0.680\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6958656273a04bb4abfdf9ced135839c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accent classification accuracy: 32.08%\n",
      "Average validation loss: 0.682\n",
      "## EPOCH 4 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9599aec4527b4363844bf86701ba7902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 average loss: 0.670\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f97ab12b554a76b278e495403f488e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accent classification accuracy: 32.08%\n",
      "Average validation loss: 0.688\n",
      "## EPOCH 5 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c51c0bca0744eb2b02078ad2463b66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 average loss: 0.675\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96e9993918e4b179a33d59e2e8372b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accent classification accuracy: 32.08%\n",
      "Average validation loss: 0.688\n",
      "## EPOCH 6 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b4875679fb46d8b9a171b9d6bf74ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 average loss: 0.673\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf71157301a4ee8aabc934beffce252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accent classification accuracy: 32.08%\n",
      "Average validation loss: 0.689\n",
      "## EPOCH 9 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e014cae2d4194b59ad7fd20db40c825e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 average loss: 0.673\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb70d48816c427792be9261e873f27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accent classification accuracy: 32.08%\n",
      "Average validation loss: 0.689\n",
      "## EPOCH 10 ##\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a466c3fab3934cd9b53f8ecd2655d39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 average loss: 0.673\n",
      "Testing:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f376569bd59423da8c394edd93ec05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accent classification accuracy: 32.08%\n",
      "Average validation loss: 0.688\n"
     ]
    }
   ],
   "source": [
    "SILENT = True\n",
    "    \n",
    "best_model = None\n",
    "prev_acc = 0\n",
    "## Train\n",
    "for epoch in range(1, param['epochs'] + 1):\n",
    "    print(f'## EPOCH {epoch} ##')\n",
    "    print(f'Training:')\n",
    "    model.train()\n",
    "    \n",
    "    # train\n",
    "    epoch_losses = []\n",
    "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs, target_accents, input_sizes = process_data(data)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(inputs, input_sizes)\n",
    "\n",
    "        loss = criterion(out, target_accents) #prbly fix that TODODODODO\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        if not SILENT:\n",
    "            print(f'Iteration {i+1}/{len(train_loader):<4}loss: {loss:0.3f}')\n",
    "        \n",
    "        # Gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        clip_grad_norm_(model.parameters(), param['max_norm'])\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss = sum(epoch_losses) / len(train_loader)\n",
    "    tb_writer.add_scalar('stats/train_loss', epoch_loss, epoch)\n",
    "    print(f'Epoch {epoch} average loss: {epoch_loss:0.3f}')\n",
    "        \n",
    "    # validate\n",
    "    print(f'Testing:')\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    tot = 0\n",
    "    with torch.no_grad():\n",
    "        epoch_val_losses = []\n",
    "        for data in tqdm(test_loader, total=len(test_loader)):\n",
    "            inputs, target_accents, input_sizes = process_data(data) \n",
    "            out = model(inputs, input_sizes)\n",
    "            \n",
    "            val_loss = criterion(out, target_accents)\n",
    "            epoch_val_losses.append(val_loss)\n",
    "            \n",
    "            for x in range(len(target_accents)):\n",
    "                accent_out = np.argmax(out[x]) # take exp because we do logsoftmax\n",
    "                accent_target = target_accents[x]\n",
    "\n",
    "                if accent_out.item() == accent_target.item():\n",
    "                    acc += 1\n",
    "                tot += 1\n",
    "\n",
    "        acc = acc / tot * 100\n",
    "        epoch_val_loss = sum(epoch_val_losses) / len(test_loader)\n",
    "        \n",
    "    tb_writer.add_scalar('stats/accuracy', acc, epoch)\n",
    "    print(f'Accent classification accuracy: {acc:0.2f}%')\n",
    "    \n",
    "    tb_writer.add_scalar('stats/val_loss', epoch_val_loss, epoch)\n",
    "    print(f'Average validation loss: {val_loss:0.3f}')\n",
    "    \n",
    "    if acc > prev_acc:\n",
    "        print('New best model found.')\n",
    "        best_model = model\n",
    "        prev_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bf578ea6bbd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "t.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a = np.argmax(out[0])\n",
    "a == torch.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6682, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(out, target_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 ms, sys: 222 ms, total: 337 ms\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_test = [process_data(d) for d in test_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 299, 1: 633, 2: 0, 3: 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = []\n",
    "[accs.extend(a) for i, a, s in all_test]\n",
    "accs = [i.item() for i in accs]\n",
    "\n",
    "cnts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "for e in accs:\n",
    "    cnts[e] = cnts[e] + 1\n",
    "    \n",
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3208154506437768, 0.6791845493562232, 0.0, 0.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = sum(cnts.values())\n",
    "[x / tot for x in cnts.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['england', 'us'], dtype='<U7')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accent_binarizer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(inputs, input_sizes)\n",
    "    out = torch.tensor([np.argmax(torch.exp(o)).item() for o in out]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(inputs, input_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0449, 0.9551],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901],\n",
       "        [0.5099, 0.4901]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['england' 'us']\n",
      "tar 0\n",
      "out 1\n",
      "#\n",
      "tar 1\n",
      "out 1\n",
      "#\n",
      "tar 0\n",
      "out 1\n",
      "#\n",
      "tar 1\n",
      "out 0\n",
      "#\n",
      "tar 0\n",
      "out 1\n",
      "#\n",
      "tar 0\n",
      "out 1\n",
      "#\n",
      "tar 1\n",
      "out 0\n",
      "#\n",
      "tar 0\n",
      "out 1\n",
      "#\n",
      "tar 1\n",
      "out 1\n",
      "#\n",
      "tar 0\n",
      "out 0\n",
      "#\n",
      "tar 1\n",
      "out 1\n",
      "#\n",
      "tar 1\n",
      "out 1\n",
      "#\n",
      "tar 0\n",
      "out 1\n",
      "#\n",
      "tar 0\n",
      "out 1\n",
      "#\n",
      "tar 1\n",
      "out 0\n",
      "#\n",
      "tar 0\n",
      "out 1\n",
      "#\n",
      "tar 1\n",
      "out 0\n",
      "#\n",
      "tar 1\n",
      "out 1\n",
      "#\n",
      "tar 0\n",
      "out 0\n",
      "#\n",
      "tar 1\n",
      "out 1\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "inputs, target_accents, input_sizes = all_test[7]\n",
    "print(accent_binarizer.classes_)\n",
    "for i in range(len(inputs)):\n",
    "    s = i\n",
    "    e = s + 1\n",
    "\n",
    "    us_in = inputs[s:e]\n",
    "    us_si = input_sizes[s:e]\n",
    "    #print(us_in)\n",
    "\n",
    "    tar_acc = target_accents[s:e]\n",
    "    print('tar', tar_acc.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(us_in, us_si)\n",
    "        res = np.argmax(torch.exp(out))\n",
    "        print('out', res.item())\n",
    "    print('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5532704"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "out, output_sizes = model(inputs, input_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2268,  0.2268],\n",
       "        [-0.5505,  0.5505]], device='cuda:0', grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(np.argmax(out, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn.parameter import Parameter\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "# from models.modules import MaskConv, SequenceWise, BatchRNN, InferenceBatchSoftmax, \\\n",
    "#                     supported_rnns, supported_rnns_inv\n",
    "\n",
    "# class AccentClassifier(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  labels,\n",
    "#                  audio_conf={}, \n",
    "#                  rnn_hidden_size=800, \n",
    "#                  nb_layers=2, \n",
    "#                  rnn_type=nn.GRU):\n",
    "        \n",
    "#         super(AccentClassifier, self).__init__()\n",
    "\n",
    "#         # metadata\n",
    "#         self._audio_conf = audio_conf\n",
    "#         self._labels = labels\n",
    "#         self._num_classes = len(labels)\n",
    "        \n",
    "#         sample_rate = self._audio_conf.get(\"sample_rate\", 16000)\n",
    "#         window_size = self._audio_conf.get(\"window_size\", 0.02)\n",
    "\n",
    "#         self.conv = MaskConv(nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Hardtanh(0, 20, inplace=True),\n",
    "#             nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.Hardtanh(0, 20, inplace=True)\n",
    "#         ))\n",
    "\n",
    "#         # Based on above convolutions and spectrogram size using conv formula (W - F + 2P)/ S+1\n",
    "#         rnn_input_size = int(math.floor((sample_rate * window_size) / 2) + 1)\n",
    "#         rnn_input_size = int(math.floor(rnn_input_size + 2 * 20 - 41) / 2 + 1)\n",
    "#         rnn_input_size = int(math.floor(rnn_input_size + 2 * 10 - 21) / 2 + 1)\n",
    "#         rnn_input_size *= 32\n",
    "\n",
    "#         rnns = []\n",
    "#         rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "#                        bidirectional=True, batch_norm=False)\n",
    "#         rnns.append(('0', rnn))\n",
    "#         for x in range(nb_layers - 1):\n",
    "#             rnn = BatchRNN(input_size=rnn_hidden_size, hidden_size=rnn_hidden_size, rnn_type=rnn_type,\n",
    "#                            bidirectional=True)\n",
    "#             rnns.append(('%d' % (x + 1), rnn))\n",
    "            \n",
    "#         self.rnns = nn.Sequential(OrderedDict(rnns))\n",
    "\n",
    "#         fully_connected = nn.Sequential(\n",
    "#             nn.BatchNorm1d(rnn_hidden_size),\n",
    "#             nn.Linear(rnn_hidden_size, self._num_classes, bias=False)\n",
    "#         )\n",
    "#         self.fc = nn.Sequential(\n",
    "#             SequenceWise(fully_connected),\n",
    "#         )\n",
    "#         self.inference_softmax = InferenceBatchSoftmax()\n",
    "\n",
    "\n",
    "#     def forward(self, x, lengths):\n",
    "#         lengths = lengths.cpu().int()\n",
    "#         output_lengths = self.get_seq_lens(lengths)\n",
    "#         x, _ = self.conv(x, output_lengths)\n",
    "#         sizes = x.size()\n",
    "#         x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # Collapse feature dimension\n",
    "#         x = x.transpose(1, 2).transpose(0, 1).contiguous()  # TxNxH\n",
    "\n",
    "#         for rnn in self.rnns:\n",
    "#             x = rnn(x, output_lengths)\n",
    "\n",
    "#         x = self.fc(x)\n",
    "#         x = x.transpose(0, 1)\n",
    "#         # identity in training mode, softmax in eval mode\n",
    "#         x = self.inference_softmax(x)\n",
    "#         return x, output_lengths\n",
    "    \n",
    "#     def get_seq_lens(self, input_length):\n",
    "#         \"\"\"\n",
    "#         Given a 1D Tensor or Variable containing integer sequence lengths, return a 1D tensor or variable\n",
    "#         containing the size sequences that will be output by the network.\n",
    "#         :param input_length: 1D Tensor\n",
    "#         :return: 1D Tensor scaled by model\n",
    "#         \"\"\"\n",
    "#         seq_len = input_length\n",
    "#         for m in self.conv.modules():\n",
    "#             if type(m) == nn.modules.conv.Conv2d:\n",
    "#                 seq_len = ((seq_len + 2 * m.padding[1] - m.dilation[1] * (m.kernel_size[1] - 1) - 1) / m.stride[1] + 1)\n",
    "#         return seq_len.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AccentClassifier(\n",
       "  (conv): MaskConv(\n",
       "    (seq_module): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=0, max_val=20, inplace)\n",
       "      (3): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=0, max_val=20, inplace)\n",
       "    )\n",
       "  )\n",
       "  (rnns): Sequential(\n",
       "    (0): BatchRNN(\n",
       "      (rnn): GRU(1312, 800, bidirectional=True)\n",
       "    )\n",
       "    (1): BatchRNN(\n",
       "      (batch_norm): SequenceWise (\n",
       "      BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
       "      (rnn): GRU(800, 800, bidirectional=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): SequenceWise (\n",
       "    Sequential(\n",
       "      (0): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=800, out_features=2, bias=False)\n",
       "    ))\n",
       "  )\n",
       "  (inference_softmax): InferenceBatchSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(10, 20, 2)\n",
    "input_ = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input_, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(20, 32, 41, 563)\n",
    "b = torch.randn(20, 32, 41, 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = a.view(a.size(0), a.size(1) * a.size(2), a.size(3))\n",
    "t = t.transpose(1, 2)\n",
    "t2 = b.view(b.size(0), b.size(1) * b.size(2), b.size(3))\n",
    "t2 = t2.transpose(1, 2)\n",
    "\n",
    "rnn = nn.GRU(32 * 41, 256, 1)\n",
    "output, hn = rnn(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 420, 256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 420, 256])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f1c4be541d8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output[:, -1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    localtime   = time.localtime()\n",
    "    timeString  = time.strftime(\"%Y-%m-%d_%Hh%M:%S\", localtime)\n",
    "    return timeString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-12-05_10h57:23'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
